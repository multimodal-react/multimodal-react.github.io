<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Prompting ChatGPT for Multimodal Reasoning and Action">
  <meta name="keywords" content="MM-ReAct, ChatGPT, GPT-4">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MM-ReAct: Prompting ChatGPT for Multimodal Reasoning and Action</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="icon" href="./static/images/icon.png">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
		/* Define the grid layout */
		.mygrid {
			display: grid;
			grid-template-columns: repeat(3, 1fr);
			grid-gap: 20px;
			width: 80%;
			margin: auto;
		}
		.grid_item {
      background: #FFFFFF;
      opacity: 1;
    }

		/* Define the size of the GIFs */
		.mygif {
			height: auto;
			cursor: pointer;
		}
		
		/* Define the modal styles */
		.modal {
			display: none;
			position: fixed;
			z-index: 1;
			left: 0;
			top: 0;
			width: 100%;
			height: 100%;
			overflow: auto;
			background-color: rgba(0,0,0,0.9);
		}
		
		.modal-content {
			margin: auto;
			display: block;
			width: 80%;
			max-width: 800px;
			max-height: 80%;
		}

    /* Define the full-screen overlay styles */
		.overlay {
			position: fixed;
			z-index: 999;
			left: 0;
			top: 0;
			width: 100%;
			height: 100%;
			overflow: hidden;
			background-color: rgba(0,0,0,0.9);
			display: none;
		}
		
		.overlay img {
			width: auto;
			height: 90%;
			margin: 0 auto;
			display: block;
			max-width: 90%;
			max-height: 90%;
		}

    /* Define the video styles */
		.gifvideo {
			width: 100%;
			height: auto;
		}

		/* Define the progress bar styles */
		.progress {
			width: 100%;
			height: 10px;
			background-color: #ddd;
			position: relative;
		}

		.progress-bar {
			height: 100%;
			background-color: #4CAF50;
			position: absolute;
			top: 0;
			left: 0;
		}
		
		/* Define the close button style */
		.close {
			color: white;
			position: absolute;
			top: 10px;
			right: 25px;
			font-size: 35px;
			font-weight: bold;
			cursor: pointer;
		}
		
		.close:hover,
		.close:focus {
			color: #bbb;
			text-decoration: none;
			cursor: pointer;
		}
	</style>
  </head>
  <body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MM-ReAct <img src="./static/images/icon.png" alt="MM-ReAct" width="50"/>:</h1>
          <h2 class="title is-2 publication-title" style="width: 110%; margin-left: -5%">Prompting ChatGPT for Multimodal Reasoning and Action </h2>
          <div class="is-size-5">
            <span class="author-block">
              <a href="https://zyang-ur.github.io/" style="color:#00A4EF;font-weight:normal;">Zhengyuan Yang<sup>*</sup></a>
            </span>, 
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en" style="color:#00A4EF;font-weight:normal;">Linjie Li<sup>*</sup></a>
            </span>, 
            <span class="author-block">
              <a href="http://jianfengwang.me/" style="color:#00A4EF;font-weight:normal;">Jianfeng Wang<sup>*</sup></a>
            </span>, 
            <span class="author-block">
              <a href="https://sites.google.com/site/kevinlin311tw/me" style="color:#00A4EF;font-weight:normal;">Kevin Lin<sup>*</sup></a>
            </span>, 
            <span class="author-block">
              <a href="https://www.linkedin.com/in/azarnasab/" style="color:#00A4EF;font-weight:normal;">Ehsan Azarnasab<sup>*</sup></a>
            </span>, 
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/fiahmed/" style="color:#00A4EF;font-weight:normal;">Faisal Ahmed<sup>*</sup></a>
            </span>, 
            <br>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/zliu/" style="color:#00A4EF;font-weight:normal;">Zicheng Liu</a>
            </span>, 
            <span class="author-block">
              <a href="https://www.linkedin.com/in/ce-liu-5697501a/" style="color:#00A4EF;font-weight:normal;">Ce Liu</a>
            </span>, 
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/nzeng/" style="color:#00A4EF;font-weight:normal;">Michael Zeng</a>
            </span>, 
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/lijuanw/" style="color:#00A4EF;font-weight:normal;">Lijuan Wang<sup>&#x2660;</sup></a>
            </span>
          </div>

          <br>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <!-- <b style="color:#008AD7; font-weight:normal">&#x25B6 </b> -->
              Microsoft Azure AI</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>*</sup>Equal Technical Contribution, </span>
            <span class="author-block"><sup>&#x2660;</sup>Project Lead </span>
           
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/microsoft/MM-REACT" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                  </a>
              </span>
              <!-- Demo Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/microsoft-cognitive-service/mm-react" target="_blank" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Demo (Coming Soon)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img id="teaser" width="120%" src="images/teaser.png">

      <h2 class="subtitle has-text-centered">
        MM-REACT allocates specialized vision experts with ChatGPT to solve challenging visual understanding tasks through multi-
modal reasoning and action.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action. In this paper, we define and explore a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models. To achieve such advanced visual intelligence, MM-REACT introduces a textual prompt design that can represent text descriptions, textualized spatial coordinates, and dense visual signals such as images and videos represented as aligned file names. MM-REACT’s prompt design allows language models to accept, associate, and process multimodal information, thereby facilitating the synergetic combination of ChatGPT and various vision experts. Zero-shot experiments demonstrate MM-REACT’s effectiveness in addressing the specified capabilities of interests and its wide application in different scenarios that require complicated visual understanding. Furthermore, we discuss and compare MM-REACT’s system paradigm with an alternative approach that extends language models for multimodal scenarios through joint finetuning. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <br>
    <br>
    <!-- Paper Model. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">MM-ReAct Design</h2>
        <div class="content has-text-justified">
          <p>
            <b>MM-ReAct is a system paradigm
              that composes numerous vision experts with ChatGPT for
              multimodal reasoning and action.</b>
          </p>
          <ul>
            <li>To enable the image as
              input, we simply use the file path as the input to ChatGPT.
              The file path functions as a placeholder, allowing ChatGPT
              to treat it as a black box. </li>
            <li>Whenever a specific property, such
              as celebrity names or box coordinates, is required, Chat-
              GPT is expected to seek help from a specific vision expert
              to identify the desired information. </li>
            <li>The expert output is serialized as text and
              combined with the input to further activate ChatGPT.</li>
            <li>If no external experts are needed, we directly return the response to the user. </li>
          </ul>
        </div>        
        <img id="model" width="100%" src="images/model_figure_2.gif">
        <p class="has-text-centered">
          Flowchart of MM-REACT to enable image understanding with ChatGPT.
        </p> 
      </div>
    </div>
    <br>
    <br>
    <!-- Paper Model. -->
    
    <!-- Paper Model 2. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">MM-ReAct's Full Execution Flow</h2>
        <div class="content has-text-justified">
          <p>
            We provide an examples of MM-REACT’s full execution flow blow. More examples can be found in our paper.
          </p>
          <ul>
            <li>The executions, highlighted by bold text, can be either a ChatGPT call (e.g.,
              “<span style="color:#949494;"><b>ChatGPT:</b></span>”) or running a selected vision expert (e.g., “<span style="color:#949494;"><b>Image Captioning</b></span>”).</li>
            <li>ChatGPT executions can be used to generate <SPAN STYLE="background-color: #FFE699">thought</SPAN> (reasoning) and <SPAN STYLE="background-color: #DEEBF7">action</SPAN> texts that allocate vision experts <img src="./static/images/icon_reason.png" alt="MM-ReAct" width="20"/>, or produce the final response to users <img src="./static/images/icon.png" alt="MM-ReAct" width="20"/>.</li>
            <li>Each ChatGPT execution takes the preceding text as input
              and generates the text leading up to the next execution (e.g., “<span style="color:#949494;">This is an image. Assistant, what objects . . . image? &lt;ImagePath&gt;</span>” for
              the first execution). </li>
            <li>Texts in <span style="color:#949494;"><b>gray</b></span> represent MM-REACT’s <SPAN STYLE="background-color: #FFE699">thoughts</SPAN> or vision experts’ <SPAN STYLE="background-color: #DEEBF7">actions</SPAN> and outputs (<SPAN STYLE="background-color: #E2F0D9">observations</SPAN>), which are invisible to users. This
              multimodal reasoning and action process occurs behind the scene to gather the necessary information for generating final responses to
              users, which are shown in <b>black</b>.</li>
          </ul>
        </div>        
        <img id="model" width="60%" src="images/figure_3.gif">
        <p class="has-text-centered">
          An example of MM-REACT’s full execution flow. 
        </p>        
        </div>
      </div>
    </div>
    <br>
    <br>
    <br>
    <!-- Paper Model 2. -->

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Video Demo</h2>
        <!-- <div class="publication-video">
          <iframe src="https://user-images.githubusercontent.com/11957155/205432860-ef646e22-15fc-4527-8769-98df83381c09.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div> -->  
        <br>
        <h3 class="subtitle has-text-centered">
            Click each panel below for the corresponding video demo.
        </h3>  
        <br>
        <div class="column is-six-fifths mygrid" >
          <div class="grid_item"><img class="mygif" src="images/math.png"></div>
          <div class="grid_item"><img class="mygif" src="images/squirrel_gpt4.png"></div>
          <div class="grid_item"><img class="mygif" src="images/spatial.png"></div>
          <div class="grid_item"><img class="mygif" src="images/recipe.png"></div>
          <div class="grid_item"><img class="mygif" src="images/receipt.png"></div>
          <div class="grid_item"><img class="mygif" src="images/table.png"></div>
          <div class="grid_item"><img class="mygif" src="images/product.png"></div>
          <div class="grid_item"><img class="mygif" src="images/celebrity.png"></div>
          <div class="grid_item"><img class="mygif" src="images/mushroom.png"></div>
          <!-- <img class="mygif" src="images/math.png">
          <img class="mygif" src="images/squirrel_gpt4.png">
          <img class="mygif" src="images/spatial.png">
          <img class="mygif" src="images/recipe.png">
          <img class="mygif" src="images/receipt.png">
          <img class="mygif" src="images/table.png">
          <img class="mygif" src="images/product.png">
          <img class="mygif" src="images/celebrity.png">
          <img class="mygif" src="images/mushroom.png"> -->
        </div>
        
        <!-- <div id="myModal" class="modal">
          <span class="close">&times;</span>
          <img class="modal-content" id="modalImg">
        </div> -->
        <div id="overlay" class="overlay">
          <span class="close">&times;</span>
          <div id="overlayContent"></div>
        </div>
      </div>
    </div>
    <br>
    <br>       
    <!--/ Paper video. -->
  </div>
</section>


<!-- <section class="section">
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Results</h2>      
        </div>
      </div>
    </div>
  <div class="container is-max-desktop">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Zero-Shot Segmentation</h2>
        <img id="teaser" width="120%" src="images/zero-shot-generic-image-segmentation.png">
        <h3 class="subtitle has-text-centered">
          Zero-shot semantic segmentation with pretrained X-Decoder on 10 settings of 7 datasets.
        </h3>                 
      </div>
    </div>

  </div>
</section> -->


<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@article{zou2022xdecoder,
  author      = {Zou, Xueyan and Dou, Zi-Yi and Yang, Jianwei and Gan, Zhe and Li, Linjie and Li, Chunyuan and Dai, Xiyang and Wang, Jianfeng and Yuan, Lu and Peng, Nanyun and Wang, Lijuan and Lee, Yong Jae and Gao, Jianfeng},
  title       = {Generalized Decoding for Pixel, Image and Language},
  publisher   = {arXiv},
  year        = {2022},
}
</code></pre>
  </div>
</section> -->

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <p>We would like to express our gratitude to <a href="https://www.microsoft.com/en-us/research/people/jfgao/" style="font-weight:normal;">Jianfeng Gao</a> for his valuable suggestions, as well as to <a href="https://jwyang.github.io/" style="font-weight:normal;">Jianwei Yang</a> for generously providing the image editing tool utilizing the <a href="https://x-decoder-vl.github.io/" style="font-weight:normal;">X-Decoder</a> framework. 
    </p>
    <br>
    <br>
    <p>
      This website is adapted from <a
      href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license"
                                          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
      Commons Attribution-ShareAlike 4.0 International License</a>.
    </p>
  </div>
</section>


<script>
  $(".grid_item").hover(function () {
    $(this).css("background", "#f2f1f1");
    }, 
    function () {
        $(this).css("background", "#FFFFFF"); 
    });

  // Get the modal element
  // var modal = document.getElementById("myModal");
  var overlay = document.getElementById("overlay");
  var span = document.getElementsByClassName("close")[0];


  // Get the image element and the close button element
  //  // display the GIF as it is
  // var img = document.getElementById("modalImg");
  // var img = document.getElementById("overlayImg");
  // Add event listeners to each GIF element
  var gifs = document.getElementsByClassName("mygif");
  for (var i = 0; i < gifs.length; i++) {
  gifs[i].addEventListener("click", function() {
      //  // display the GIF as it is
      // // Set the modal image source and display the modal
      // img.src = this.src;

      // display the GIF as a new image, will play from the begining
      var img = document.createElement("img");
      img.src = this.src.replace(".png", ".gif");

      // Add the img element to the overlay content and display the overlay
      document.getElementById("overlayContent").appendChild(img);
      

      // modal.style.display = "block";
      overlay.style.display = "block";

      // Hide the body overflow
              document.body.style.overflow = "hidden";
  });
  }

  // Add event listener to close button
  span.addEventListener("click", function() {
  // Remove the img element from the overlay content, hide the overlay, and restore the body overflow
          document.getElementById("overlayContent").innerHTML = "";

  // Hide the modal
  // modal.style.display = "none";
  overlay.style.display = "none";
  document.body.style.overflow = "auto";
  });
</script>
</body>
</html>
